{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e55024f6-48aa-46ae-ae28-c8e342b41d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/testenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "from transformers import set_seed\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81a58130-e8c9-4e5a-8080-8e0a90e98171",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "val_df = pd.read_csv(\"../data/validation.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56356a8e-bc73-432d-902e-70f7c6125df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ba9fa14-53a1-44c4-9d13-1baed5ba3c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences, max_length=100, padding='max_length'):\n",
    "    return tokenizer(\n",
    "        sentences,\n",
    "        truncation=True,\n",
    "        padding=padding,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"tf\" \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac239be7-6e4d-4ceb-854d-2031c1c9db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_x_train = train_df[\"Comment_Adj\"].tolist()\n",
    "bert_y_train = train_df[\"Result_Bin\"].tolist()\n",
    "bert_x_val = val_df[\"Comment_Adj\"].tolist()\n",
    "bert_y_val = val_df[\"Result_Bin\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c468de7-c416-4ec6-8a86-0984f0671c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenize(bert_x_train)\n",
    "val_encodings = tokenize(bert_x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4ace8c8-91e6-4a5d-bc5e-2a2dccfebcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tf.convert_to_tensor(bert_y_train, dtype=tf.int32)\n",
    "val_labels = tf.convert_to_tensor(bert_y_val, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "783c1b81-4d0b-40bb-9c05-3faa24a9c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "set_seed(seed_value)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),  \n",
    "    train_labels\n",
    ")).shuffle(1000).batch(30).prefetch(1)\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),  \n",
    "    val_labels\n",
    ")).batch(30).prefetch(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbd73a3-094c-4a8a-a45b-8ce8aecdcd12",
   "metadata": {},
   "source": [
    "### Without Custom Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d46d7bd2-808e-415b-878b-5e0146cb2705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "228/228 [==============================] - 976s 4s/step - loss: 0.6641 - accuracy: 0.5987 - val_loss: 0.6537 - val_accuracy: 0.5980\n",
      "Epoch 2/2\n",
      "228/228 [==============================] - 1016s 4s/step - loss: 0.5443 - accuracy: 0.7240 - val_loss: 0.6240 - val_accuracy: 0.6652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29ef32190>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_value = 42\n",
    "set_seed(seed_value)\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=2)\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=5e-5)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics = [\"accuracy\")\n",
    "\n",
    "model.fit(\n",
    "    x=train_dataset,\n",
    "    y=None,\n",
    "    validation_data=validation_dataset,\n",
    "    batch_size=30,\n",
    "    epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4882ab2-48f9-472b-b5eb-b817f48c7418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 13:32:35.788501: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [2276,100]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 93s 1s/step - loss: 0.6230 - accuracy: 0.6586\n",
      "Loss: 0.622992217540741\n",
      "Accuracy: 0.6586115956306458\n"
     ]
    }
   ],
   "source": [
    "bert_x_test = test_df[\"Comment_Adj\"].tolist()\n",
    "bert_y_test = test_df[\"Result_Bin\"].tolist()\n",
    "\n",
    "test_encodings = tokenize(bert_x_test)\n",
    "\n",
    "test_labels = tf.convert_to_tensor(bert_y_test, dtype=tf.int32)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),  \n",
    "    test_labels\n",
    ")).shuffle(1000).batch(30).prefetch(1)\n",
    "\n",
    "results = model.evaluate(test_dataset)\n",
    "print(\"Loss:\", results[0])\n",
    "print(\"Accuracy:\", results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3594680-3df5-402a-a4d8-55d1cc37986e",
   "metadata": {},
   "source": [
    "### With Custom Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d8de98b-5d6a-44e7-b2c0-df3bea554231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 14:24:07.122147: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype int32 and shape [6826]\n",
      "\t [[{{node Placeholder/_2}}]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"/opt/anaconda3/envs/testenv/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/testenv/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/envs/testenv/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/anaconda3/envs/testenv/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1670, in train_step\n        if list(y_pred.keys())[0] == \"loss\":\n\n    AttributeError: 'Tensor' object has no attribute 'keys'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 66\u001b[0m\n\u001b[1;32m     59\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mlegacy\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n\u001b[1;32m     60\u001b[0m model_1\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     61\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     62\u001b[0m     loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     63\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m] \n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 66\u001b[0m \u001b[43mmodel_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m     71\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/testenv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/hs/br_4rpdj68nc3sfdpgv0xgn80000gn/T/__autograph_generated_fileu1okb462.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/testenv/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:1670\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1667\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m y_pred[: \u001b[38;5;28mlen\u001b[39m(y)]  \u001b[38;5;66;03m# Remove unused fields in case those cause problems\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1669\u001b[0m     \u001b[38;5;66;03m# If the labels are a single tensor, match them to the first non-loss tensor in the output\u001b[39;00m\n\u001b[0;32m-> 1670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(y_pred\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1671\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m y_pred[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1672\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    File \"/opt/anaconda3/envs/testenv/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/testenv/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/envs/testenv/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/anaconda3/envs/testenv/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1670, in train_step\n        if list(y_pred.keys())[0] == \"loss\":\n\n    AttributeError: 'Tensor' object has no attribute 'keys'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFDistilBertForSequenceClassification, TFDistilBertModel, DistilBertConfig\n",
    "\n",
    "seed_value = 42\n",
    "set_seed(seed_value)\n",
    "\n",
    "def tokenize(sentences, max_length=100, padding='max_length'):\n",
    "    return tokenizer(\n",
    "        sentences,\n",
    "        truncation=True,\n",
    "        padding=padding,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"tf\" \n",
    "    )\n",
    "\n",
    "bert_x_train = train_df[\"Comment_Adj\"].tolist()\n",
    "bert_y_train = train_df[\"Result_Bin\"].tolist()\n",
    "bert_x_val = val_df[\"Comment_Adj\"].tolist()\n",
    "bert_y_val = val_df[\"Result_Bin\"].tolist()\n",
    "\n",
    "train_encodings = tokenize(bert_x_train)\n",
    "val_encodings = tokenize(bert_x_val)\n",
    "\n",
    "train_labels = tf.convert_to_tensor(bert_y_train, dtype=tf.int32)\n",
    "val_labels = tf.convert_to_tensor(bert_y_val, dtype=tf.int32)\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),  \n",
    "    train_labels\n",
    ")).shuffle(1000).batch(30).prefetch(1)\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),  \n",
    "    val_labels\n",
    ")).batch(30).prefetch(1)\n",
    "\n",
    "class CustomBertClassification(TFDistilBertForSequenceClassification):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super(CustomBertClassification, self).__init__(config, *args, **kwargs)\n",
    "        self.distilbert = TFDistilBertModel(config)\n",
    "        self.pre_classifier = tf.keras.layers.Dense(768, activation='relu')\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "        self.classifier = tf.keras.layers.Dense(config.num_labels, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False, **kwargs):\n",
    "        outputs = self.distilbert(inputs, **kwargs)\n",
    "        hidden_state = outputs[0]  # Get the last hidden state\n",
    "        pooled_output = hidden_state[:, 0]  # Use the first token for classification\n",
    "        pooled_output = self.pre_classifier(pooled_output)\n",
    "        pooled_output = self.dropout(pooled_output, training=training)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return {'logits': logits} if not training else logits\n",
    "\n",
    "configuration = DistilBertConfig.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "model_1 = CustomBertClassification(configuration)\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=5e-5)\n",
    "model_1.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'] \n",
    ")\n",
    "\n",
    "model_1.fit(\n",
    "    x=train_dataset, \n",
    "    validation_data=validation_dataset,  \n",
    "    batch_size=30,\n",
    "    epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63725b1f-dcab-4bcc-a067-84f452145d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a4b7900-6203-409a-b89d-94129380faaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"/opt/anaconda3/envs/testenv/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/testenv/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/envs/testenv/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/anaconda3/envs/testenv/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1670, in train_step\n        if list(y_pred.keys())[0] == \"loss\":\n\n    AttributeError: 'Tensor' object has no attribute 'keys'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      3\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m      4\u001b[0m     loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      5\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m] \n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/testenv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/hs/br_4rpdj68nc3sfdpgv0xgn80000gn/T/__autograph_generated_fileu1okb462.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/testenv/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:1670\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1667\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m y_pred[: \u001b[38;5;28mlen\u001b[39m(y)]  \u001b[38;5;66;03m# Remove unused fields in case those cause problems\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1669\u001b[0m     \u001b[38;5;66;03m# If the labels are a single tensor, match them to the first non-loss tensor in the output\u001b[39;00m\n\u001b[0;32m-> 1670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(y_pred\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1671\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m y_pred[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1672\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    File \"/opt/anaconda3/envs/testenv/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/testenv/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/envs/testenv/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/anaconda3/envs/testenv/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1670, in train_step\n        if list(y_pred.keys())[0] == \"loss\":\n\n    AttributeError: 'Tensor' object has no attribute 'keys'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f91f66b-6236-4085-a393-936d63985bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b0617d-4185-498e-a301-a204f067004b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46c938-b472-4d91-b507-8c59a2afdade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ee5f7a-03f1-45fb-862e-fad1bba29444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (testenv)",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
