{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9a807a6a-bfb2-44f7-ac15-24043f0b388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import BertForSequenceClassification, AdamW  \n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "883141b4-cd6d-45d0-ad6b-caa3b905ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "val_df = pd.read_csv(\"../data/validation.csv\")\n",
    "test_df = pd.read_csv(\"../data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74ac3c58-156e-4f6e-9fa0-a522f4f21449",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('prajjwal1/bert-mini')\n",
    "\n",
    "def tokenize(data, max_length=87):\n",
    "    return tokenizer(\n",
    "        data[\"Comment_Adj\"].tolist(),\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "class CommentsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_encodings = tokenize(train_df)\n",
    "val_encodings = tokenize(val_df)\n",
    "test_encodings = tokenize(test_df)\n",
    "\n",
    "train_dataset = CommentsDataset(train_encodings, train_df['Result_Bin'])\n",
    "val_dataset = CommentsDataset(val_encodings, val_df['Result_Bin'])\n",
    "test_dataset = CommentsDataset(test_encodings, test_df['Result_Bin'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f59a5-371e-4c5c-9c6c-6a60448bf365",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('prajjwal1/bert-mini', num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "total_epochs = 6\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    with tqdm(train_loader, unit=\"batch\", desc=f\"Epoch {epoch + 1}/{total_epochs}\") as pbar:\n",
    "        for batch in pbar:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "print(\"Training completed.\")\n",
    "\n",
    "model.save_pretrained('./bert_pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9832a48-1547-4b1f-a778-34da13e88456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hs/br_4rpdj68nc3sfdpgv0xgn80000gn/T/ipykernel_6841/2057042811.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision on Test: 0.657\n",
      "Recall on Test: 0.705\n",
      "F1 Score on Test: 0.68\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        predictions.extend(torch.argmax(logits, dim=-1).tolist())\n",
    "        true_labels.extend(batch['labels'].tolist())\n",
    "\n",
    "precision = precision_score(true_labels, predictions)\n",
    "recall = recall_score(true_labels, predictions)\n",
    "f1 = f1_score(true_labels, predictions)\n",
    "print(\"Precision on Test:\", round(precision,3))\n",
    "print(\"Recall on Test:\", round(recall,3))\n",
    "print(\"F1 Score on Test:\", round(f1,3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
